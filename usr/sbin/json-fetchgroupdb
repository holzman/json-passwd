#!/usr/bin/python2
# Generate /var/db/groups.db from a remote JSON source.

#########################################################################
### Configuration #######################################################
#########################################################################

import filecmp, os

if 'JPCONF' in os.environ:
    configFile = os.environ['JPCONF']
else:
    configFile = '/etc/json-passwd/config'

#########################################################################
### Declarations ########################################################
#########################################################################

import datetime, json, optparse, os.path, re, requests, shlex, \
    shutil, subprocess, sys, syslog, tempfile, time
from distutils.version import StrictVersion

requiredElements = ['gid', 'users']

tmpDir = tempfile.mkdtemp()

#########################################################################
### Subroutines #########################################################
#########################################################################

def cmd_external_stderr_to_return(cmdline):
    """
    Run a command with subprocess.Popen(), and send the first line of
    STDERR as a return value, and leave STDOUT to go to actual STDOUT.
    """
    proc = subprocess.Popen(shlex.split(cmdline.encode('ascii')),
        stderr=subprocess.PIPE)
    (out, err) = proc.communicate()
    value = err.rstrip()
    return value

def loggit(severity, message):
    """
    Write out error messages to our chosen locations.  If we get a
    severity of 'ERROR', then also exit with status code 1.
    """
    mypid = os.getpid()
    ts = datetime.datetime.fromtimestamp(
        time.time()).strftime('%Y-%m-%d_%H:%M:%S')
    loglocation = opt.logFile
    with open(loglocation, 'a+') as text_file:
        text_file.write(('[%s] %s %s: %s\n') % (mypid, ts, severity, message))
    if opt.debug:
        print ('[%s] %s %s: %s') % (mypid, ts, severity, message)
    if severity == 'NOTICE':
        sys.stderr.write('%s: %s\n' % (severity, message))
    if severity == 'SUCCESS':
        sys.stderr.write('%s: %s\n' % (severity, message))
        syslog.syslog(syslog.LOG_INFO, "%s - %s: %s"
            % (os.path.basename(__file__), severity, message))
    if severity == 'ERROR':
        sys.stderr.write('%s: %s\n' % (severity, message))
        syslog.syslog(syslog.LOG_INFO, "%s - %s: %s"
            % (os.path.basename(__file__), severity, message))
        shutil.rmtree(tmpDir)
        sys.exit(1)

def parseConfig(file):
    """
    Load a json configuration from a configuration file.  Sets a global
    'config' variable.
    """
    global config

    try:
        config = json.load(open(file, 'r'))
    except IOError, e:
        print "file error:  %s" % e
        sys.exit(2)
    except Exception, e:
        print "unknown error:  %s" % e
        sys.exit(2)

    return config


def parseGroups(data):
    """

    Look at the 'groups' data from the json data, and generates a list of
    group entries:

        NAME:x:GID:USER,USER,USER,...

    Returns two items: an array of matching entries, and a count of
    entries that couldn't be parsed.
    """
    loggit('INFO', 'Processing groups list')
    shortlist = []
    newDict = {}
    skip = {}

    groups = data['groups']
    success = []
    numFailures = 0

    if 'groupIgnore' in config:
        for i in config['groupIgnore']:
            skip[i] = 1

    for groupname in groups:
        shortDict = groups[groupname]
        keyFailures = []
        for requiredElement in requiredElements:
            if requiredElement not in shortDict:
                keyFailures.append(requiredElement)

        if len(keyFailures) > 0:
            loggit('NOTICE', '%s: missing key(s): %s' % (groupname,
                ','.join(keyFailures)))
            numFailures += 1

        else:
            gid = str(shortDict['gid'])
            users = shortDict['users']

            groupEntry = '%s:*:%s:%s' % \
                (groupname, gid, users)

            if groupname in skip:
                loggit('NOTICE', '%s - skipping group (groupIgnore)' % groupname)
                numFailures += 1

            elif (re.search('^[a-z0-9_-]+$', groupname)
              and re.search('^[0-9]+$', gid)
              and isinstance(users, basestring)):
                shortlist.append(gid)

                if gid not in newDict:
                    newDict[gid] = []
                newDict[gid].append(groupEntry)
            else:
                loggit('NOTICE', '%s - could not parse (%s)'
                    % (groupname, shortDict))
                numFailures += 1

    shortlist = list(set(shortlist))
    shortlist.sort(key=int)
    for gid in shortlist:
        e = newDict[gid]
        for i in e:
            success.append(i)

    return success, numFailures

def populateGroupsFile(data, success):
    """
    Populate /var/db/groups.db (or equivalent).  We do a variety of
    checks to confirm that everything wrote out properly before we
    actually drop the file in place.
    """

    ts = datetime.datetime.fromtimestamp(
        data['generationTime']).strftime('%Y%m%d_%H-%M-%S')

    tmpFile = '%s/sanitized-group-%s.txt' % (tmpDir, ts)
    tmpDb = '%s/sanitized-group-%s.db' % (tmpDir, ts)
    tmpDb2 = '%s/temp_group.db' % (opt.workDir)
    make = config['makeFile']

    final = opt.db
    finalBak = final + '.bak'

    loggit('INFO', '%s: creating file and populating' % tmpFile)
    with open(tmpFile, 'w') as text_file:
        for item in success:
            text_file.write('%s\n' % item)

    loggit('INFO', '%s: validating' % tmpFile)
    if os.path.isfile(tmpFile):
        num_lines = sum(1 for line in open(tmpFile))
        if num_lines == len(success):
            loggit('INFO', '%s: correct number of users (%s)'
                % (tmpFile, len(success)))
        else:
            loggit('ERROR', '%s: contains %s entries, should be %s'
                % (tmpFile, num_lines, len(success)))
    else:
        loggit('ERROR', '%s: file not found' % (tmpFile))

    cmd = '/usr/bin/make -f %s %s/group.db VAR_DB=%s SOURCE=%s OUTPUT=%s' \
        % (make, opt.workDir, opt.workDir, tmpFile, tmpDb)
    loggit('INFO', '%s: generating compiled DB file: %s' % (tmpDb, cmd))
    error = cmd_external_stderr_to_return(cmd)
    if error != '':
        loggit('ERROR', 'command failed:\n# %s\n%s' % (cmd, error))

    if validateMakeDb(tmpDb, success):
        pass
    else:
        loggit('ERROR', 'unknown error when validating with makedb')

    loggit('INFO', '%s: moving to %s' % (tmpDb, tmpDb2))
    shutil.move(tmpDb, tmpDb2)

    if filecmp.cmp(tmpDb2, final):
        loggit('INFO', '%s and %s are identical, no updating' % (tmpDb2, final))
        return

    if os.path.isfile(final):
        loggit('INFO', 'cp %s %s' % (final, finalBak))
        shutil.copy2(final, finalBak)

    loggit('INFO', 'cp %s %s' % (tmpDb2, final))
    shutil.copy2(tmpDb2, final)

    if validateGetEnt(success):
        pass
    else:
        recoverBackup(final, finalBak)

    loggit('INFO', '%s: database is complete' % (final))

def proxyOrEmpty():
    """
    Support proxies; assumes that the entry in the config file is
    accurate.
    """

    if 'proxies' in config: return config['proxies']
    else: return {}

def recoverBackup(final, finalBak):
    """
    Recover the output data file from backup.
    """
    if os.path.isfile(finalBak):
        loggit('NOTICE', '%s: restoring from %s' % (final, finalBak))
        shutil.copy2(finalBak, final)
    else:
        loggit('NOTICE', '%s: %s does not exist' % (final, finalBak))

def retrieveJson(url):
    """
    Pull down a json-formatted file from a URL, and returns a requestUrl
    data object.
    """
    try:
        requestURL = requests.get(url, verify=False, timeout=4.00,
            proxies=proxyOrEmpty())
        if StrictVersion(requests.__version__) > StrictVersion('1.0.0'):
            data = requestURL.json()
        else:
            data = requestURL.json
    except:
        loggit('ERROR', 'failure while retrieving json data from URL (%s)'
            % (url))

    loggit('INFO', 'retrieved URL (%s)' % (url))
    return data

def validateGetEnt(entries):
    """
    Confirm that the number of group entries from the 'db' type in nss
    matches the number of entries we should have created.  If not, restore
    from the backup file.
    """

    if opt.db != '/var/db/group.db':
        loggit('INFO', 'getent will not work unless we are looking at /var/db/group.db')
        return True

    loggit('INFO', 'group.db: validate using getent')
    cmd = '/usr/bin/getent -s db group'
    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)
    (stdout, stderr) = p.communicate()
    if p.wait() != 0:
        loggit('NOTICE', 'group.db: error when trying to use getent')
        return False

    if len(stdout.splitlines()) == len(entries):
        loggit('INFO', 'group.db: contains %s entries (appears correct)'
            % (len(entries)))
        return True
    else:
        loggit('NOTICE', 'group.db: contains %s entries (should be %s)'
            % (len(stdout.splitlines()), len(entries)))
        return False

    return True

def validateMakeDb(file, entries):
    """
    Confirm that the group data looks good, according to 'makedb -u'.
    """
    loggit('INFO', '%s: verifying contents' % (file))
    linecount = 0
    cmd = '/usr/bin/makedb -u %s' % (file)
    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)
    (stdout, stderr) = p.communicate()
    if p.wait() != 0:
        loggit('ERROR', '%s: could not verify by makedb' % (file))
    for line in stdout.splitlines():
        if re.search('^\=[0-9]+ .*', line):
            linecount += 1

    if linecount != len(entries):
        loggit('ERROR', '%s: contains %s entries, should be %s'
            % (file, linecount, len(entries)))
    else:
        loggit('INFO', '%s: contains %s entries (appears correct)'
            % (file, linecount))

    return True

def validateMetadata(data):
    """
    Confirms that required fields are all present in the json data.
    Returns an error string or, if everything is okay, an empty string.
    """
    if 'generationTime' in data:
        if isinstance(data['generationTime'], float) or \
           isinstance(data['generationTime'], int):
            pass
        else:
            return 'generationTime: must be a float or an int'
    else:
        return 'generationTime: required field not present'

    if 'numberOfGroups' in data:
        if isinstance(data['numberOfGroups'], int):
            if data['numberOfGroups'] <= 0:
                return 'numberOfGroups: must be greater than 0'
        else:
            return 'numberOfGroups: must be an integer'
    else:
        return 'numberOfGroups: required field not present'

    if 'groups' in data: pass
    else:
        return 'groups: required hash is empty'

    return ''


#########################################################################
### main () #############################################################
#########################################################################

def main():
    parseConfig(configFile)

    usage = "%prog [options]"
    p = optparse.OptionParser(usage=usage,
        description="fetch group database data, write to group.db")
    p.add_option('--db', dest='db', action='store',
        default=config['groupFile'], help='%default')
    p.add_option('--debug', dest='debug', action='store_true',
        help='set to print debugging information')
    p.add_option('--logFile', dest='logFile', action='store',
        default=config['logFile'], help='%default')
    p.add_option('--test', dest='test', action='store_true', default=False,
        help='if set, print output to STDOUT; default: %default')
    p.add_option('--url', dest='url', action='store',
        default=config['groupUrl'], help='%default')
    p.add_option('--workDir', dest='workDir', action='store',
        default=config['workDir'], help='%default')

    global opt
    opt, args = p.parse_args()

    data = retrieveJson(opt.url)
    try:
        error = validateMetadata(data)
        if error != '': loggit('ERROR', error)

        (success, numFailures) = parseGroups(data)
        numSuccess = len(success)

        loggit('INFO', 'Number of good groups [%s]' % (numSuccess))
        loggit('INFO', 'Number of failed groups [%s]' % (numFailures))

        totalDetectedGroups = numSuccess + numFailures
        if totalDetectedGroups == data['numberOfGroups']:
            if (opt.test):
                for i in success: print i
            else:
                populateGroupsFile(data, success)
                loggit('SUCCESS', "%s - good [%s], failed [%s]"
                    % (opt.db, numSuccess, numFailures))

        else:
            loggit('ERROR',
                "group count (%s) does not match 'numberOfGroups' (%s)"
                % (totalDetectedGroups, data['numberOfGroups']))

        shutil.rmtree(tmpDir)
        sys.exit(0)

    except Exception, e:
        loggit('ERROR', '(Python Error) %s' % (str(e)))

if __name__ == "__main__":
    main()

#########################################################################
### POD Documentation ###################################################
#########################################################################
## We use this to generate man pages.

"""

=head1 NAME

json-fetchgroupdb - populate /var/db/groups.db from external json data

=head1 SYNOPSIS

B<json-fetchgroupdb> [options]

=head1 USAGE

json-fetchgroupdb pulls an external JSON-formatted list of Unix groups,
converts it to a .db file suitable to play nicely with nss_db, and copies
it into place.  There are a variety of checks along the way to make sure
everything is working nicely.

=head1 OPTIONS

=over 4

=item I<--db> F<FILE>

Override the file we should run against.  Default: /var/db/group.db

=item I<--debug>

If set, prints debugging information to STDOUT.

=item I<--logFile> F<FILE>

Where should we send log messages?  Default: /var/log/group-db.log

=item I<--test>

If set, print logging output to STDOUT as well.  Default: false.

=item I<--url> I<URL>

From where should we retrieve the data?  Default is in the config file.

=item I<--workDir> F<DIRECTORY>

Where should we store files by default?  Default is in the config file.

=back

=head1 INPUT

The JSON should contain the following fields:

=over 4

=item generationTime

What time was this data generated?  Contents: int or float,
number-of-seconds since epoch.

=item numberOfGroups

How many entries should be in the 'groups' field?  Contents: integer.

=item groups

A hash of group entries.  The key for each value is the group name (must
be a parseable string); each entry should contain:

=over 4

=item gid

String containing the numeric gid.

=item users

String containing all user entries.  Can be empty.

=back

=back

=head1 FILES

=over 4

=item F</etc/json-passwd/config>

JSON-formatted configuration file.  Can be overridden with the environment
variable I<JPCONF>.

=back

=head1 AUTHOR

Tim Skirvin <tskirvin@fnal.gov>

Tyler Parsons <tparsons@fnal.gov>

=head1 COPYRIGHT

Copyright 2015-2021, Fermi National Accelerator Laboratory

This program is free software; you may redistribute it and/or modify
it under the same terms as Perl itself.

=cut

"""
